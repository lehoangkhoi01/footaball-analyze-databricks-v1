{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f8b027-6477-4e95-a219-743eeb69cf60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `football-analyze-v1`;\n",
    "USE SCHEMA `football`;\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09a55f8-f93f-4d49-bb10-5e4ba9c8d1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from src.api.api_handler import APIError, APIRequestHandler\n",
    "from src.api.endpoints import LEAGUES_ENDPOINT\n",
    "from src.schemas.fields import TableNames, CommonFields, LeagueFields\n",
    "from src.schemas.league_schema import LeagueSchema\n",
    "from pyspark.sql.types import LongType\n",
    "from src.schemas.schema_validation import SchemaValidation, ValidationResult\n",
    "from src.utils.data_utils import DataUtils\n",
    "\n",
    "\n",
    "\n",
    "def flatten_dataframe(df: DataFrame, separator: str = \"_\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Recursively flatten a DataFrame with nested structs and arrays.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with nested structures\n",
    "        separator: String to separate nested field names (default: \"_\")\n",
    "    \n",
    "    Returns:\n",
    "        Flattened DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_nested_columns(schema, prefix=\"\"):\n",
    "        \"\"\"Extract all nested column paths from schema\"\"\"\n",
    "        columns = []\n",
    "        \n",
    "        for field in schema.fields:\n",
    "            field_name = f\"{prefix}{separator}{field.name}\" if prefix else field.name\n",
    "            \n",
    "            if isinstance(field.dataType, StructType):\n",
    "                # Recursively get nested struct columns\n",
    "                columns.extend(get_nested_columns(field.dataType, field_name))\n",
    "            elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):\n",
    "                # Handle array of structs - we'll explode these\n",
    "                columns.append((field_name, \"array_struct\"))\n",
    "            else:\n",
    "                # Regular column\n",
    "                columns.append((field_name, \"regular\"))\n",
    "        \n",
    "        return columns\n",
    "    \n",
    "    def flatten_struct_columns(df, separator=\"_\"):\n",
    "        \"\"\"Flatten struct columns by selecting nested fields\"\"\"\n",
    "        select_exprs = []\n",
    "        \n",
    "        for field in df.schema.fields:\n",
    "            if isinstance(field.dataType, StructType):\n",
    "                # Flatten struct fields\n",
    "                for nested_field in field.dataType.fields:\n",
    "                    alias_name = f\"{field.name}{separator}{nested_field.name}\"\n",
    "                    select_exprs.append(col(f\"{field.name}.{nested_field.name}\").alias(alias_name))\n",
    "            elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):\n",
    "                # Keep array of structs as is for now - will handle in next step\n",
    "                select_exprs.append(col(field.name))\n",
    "            else:\n",
    "                # Regular column\n",
    "                select_exprs.append(col(field.name))\n",
    "        \n",
    "        return df.select(*select_exprs)\n",
    "    \n",
    "    def has_nested_structs(df):\n",
    "        \"\"\"Check if DataFrame still has nested structs\"\"\"\n",
    "        for field in df.schema.fields:\n",
    "            if isinstance(field.dataType, StructType):\n",
    "                return True\n",
    "            elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    # Iteratively flatten until no more nested structures\n",
    "    current_df = df\n",
    "    max_iterations = 10  # Prevent infinite loops\n",
    "    iteration = 0\n",
    "    \n",
    "    while has_nested_structs(current_df) and iteration < max_iterations:\n",
    "        # First, explode any arrays of structs\n",
    "        array_columns = [field.name for field in current_df.schema.fields \n",
    "                        if isinstance(field.dataType, ArrayType) and \n",
    "                        isinstance(field.dataType.elementType, StructType)]\n",
    "        \n",
    "        for array_col in array_columns:\n",
    "            # Explode array and flatten resulting structs\n",
    "            other_cols = [col(field.name) for field in current_df.schema.fields if field.name != array_col]\n",
    "            \n",
    "            # Use explode_outer to handle null arrays\n",
    "            exploded_df = current_df.select(*other_cols, explode_outer(col(array_col)).alias(f\"{array_col}_exploded\"))\n",
    "            \n",
    "            # Now flatten the exploded struct\n",
    "            if exploded_df.schema[f\"{array_col}_exploded\"].dataType.__class__.__name__ == 'StructType':\n",
    "                struct_cols = []\n",
    "                for nested_field in exploded_df.schema[f\"{array_col}_exploded\"].dataType.fields:\n",
    "                    alias_name = f\"{array_col}{separator}{nested_field.name}\"\n",
    "                    struct_cols.append(col(f\"{array_col}_exploded.{nested_field.name}\").alias(alias_name))\n",
    "                \n",
    "                other_cols_final = [col(field.name) for field in exploded_df.schema.fields if field.name != f\"{array_col}_exploded\"]\n",
    "                current_df = exploded_df.select(*other_cols_final, *struct_cols)\n",
    "            else:\n",
    "                current_df = exploded_df.withColumnRenamed(f\"{array_col}_exploded\", array_col)\n",
    "        \n",
    "        # Then flatten any remaining struct columns\n",
    "        current_df = flatten_struct_columns(current_df, separator)\n",
    "        iteration += 1\n",
    "    \n",
    "    return current_df\n",
    "\n",
    "\n",
    "\n",
    "bronze_league_df = spark.read.table(f'staging.{TableNames.STAGING_LEAGUES}').select(\n",
    "        col(\"record_id\"),\n",
    "        from_json(col(\"json_data\"), schema=LeagueSchema.get_bronze_schema()).alias(\"parsed_data\"),\n",
    "        col(\"ingestion_timestamp\")\n",
    "    )\n",
    "\n",
    "# Extract the parsed_data fields to remove the prefix\n",
    "parsed_fields_df = bronze_league_df.select(\n",
    "        col(\"record_id\"),\n",
    "        col(\"parsed_data.*\"),  # This expands all fields from parsed_data without the prefix\n",
    "        col(\"ingestion_timestamp\")\n",
    "    )\n",
    "\n",
    "# Flatten the DataFrame\n",
    "flattened_df = DataUtils.flatten_dataframe(parsed_fields_df, separator=\"_\")\n",
    "\n",
    "flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855728ec-5c9b-46c5-b662-8259060ce560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from src.api.api_handler import APIError, APIRequestHandler\n",
    "from src.api.endpoints import LEAGUES_ENDPOINT\n",
    "from src.schemas.fields import TableNames, CommonFields, LeagueFields\n",
    "from src.schemas.league_schema import LeagueSchema\n",
    "from pyspark.sql.types import LongType\n",
    "from src.schemas.schema_validation import SchemaValidation, ValidationResult\n",
    "\n",
    "\n",
    "# spark.sql(\"USE SCHEMA `bronze`\")\n",
    "bronze_league_df = spark.read.table(f'bronze.leagues_raw_data').select(\n",
    "    col(\"record_id\"),\n",
    "    from_json(col(\"json_data\"), schema=LeagueSchema.get_bronze_schema()).alias(\"parsed_data\"),\n",
    "    col(\"ingestion_timestamp\")\n",
    ")\n",
    "\n",
    "bronze_league_df = bronze_league_df.filter(\n",
    "    col(\"parsed_data.league.id\").isNotNull() &\n",
    "    col(\"parsed_data.league.name\").isNotNull() &\n",
    "    col(\"parsed_data.league.type\").isNotNull() &\n",
    "    col(\"parsed_data.league.logo\").isNotNull() &\n",
    "    col(\"parsed_data.country.name\").isNotNull() &\n",
    "    col(\"parsed_data.country.flag\").isNotNull()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "silver_league_df = (bronze_league_df\n",
    "                        .select(\n",
    "                            col(\"parsed_data.league.id\").cast(LongType()).alias(CommonFields.LEAGUE_ID),\n",
    "                            col(\"parsed_data.league.name\").cast(StringType()).alias(LeagueFields.LEAGUE_NAME),\n",
    "                            col(\"parsed_data.league.type\").cast(StringType()).alias(LeagueFields.TYPE),\n",
    "                            col(\"parsed_data.league.logo\").cast(StringType()).alias(LeagueFields.LOGO),\n",
    "                            col(\"parsed_data.country.name\").cast(StringType()).alias(LeagueFields.COUNTRY),\n",
    "                            col(\"parsed_data.country.flag\").cast(StringType()).alias(LeagueFields.COUNTRY_FLAG)\n",
    "                        )\n",
    "\n",
    "                        # Remove rows with null values for non-nullable columns\n",
    "                        .filter(col(CommonFields.LEAGUE_ID).isNotNull())\n",
    "                        .filter(col(LeagueFields.LEAGUE_NAME).isNotNull())\n",
    "                        .filter(col(LeagueFields.TYPE).isNotNull()))\n",
    "\n",
    "silver_league_df = silver_league_df.withColumn(\n",
    "        \"valid_schema\",\n",
    "        lit(True)\n",
    "    )\n",
    "\n",
    "\n",
    "validation_results: ValidationResult = SchemaValidation.validate_schema_and_data_quality(\n",
    "    silver_league_df,\n",
    "    LeagueSchema.get_silver_schema(),\n",
    "    TableNames.SILVER_LEAGUES)\n",
    "print(validation_results.to_str())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8360773638402368,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_league_raw",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
